{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library import \n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_character_stat(description_path):\n",
    "    info = {}\n",
    "    with open(description_path) as csv_file:\n",
    "        csv_lines = csv.DictReader(csv_file)\n",
    "        for row in csv_lines:\n",
    "            image = row['image']\n",
    "            label = ord(row['label'])-ord('0')\n",
    "            if info.get(label) is None:\n",
    "                info[label] = []\n",
    "            info[label].append(image)\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def division_train_test(image_file_path, description_path):\n",
    "    info = read_character_stat(description_path)\n",
    "    train, test = 50, 5\n",
    "    division = {}\n",
    "    for key in info:\n",
    "        random.shuffle(info[key])\n",
    "        division[key]={}\n",
    "        division[key]['train'] = []\n",
    "        division[key]['test'] = [] \n",
    "        for i in range(0, test):\n",
    "            division[key]['test'].append(info[key][i])\n",
    "        for i in range(test, len(info[key])):\n",
    "            division[key]['train'].append(info[key][i])\n",
    "        # print(key, type(key),len(division[key]['train']), len(division[key]['test']))\n",
    "    return division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_file_path = os.path.join('.','datasets','Img')\n",
    "description_path = os.path.join('.','datasets','english.csv')\n",
    "division = division_train_test(image_file_path, description_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_values(np_arr):\n",
    "    return np.unique(np_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and show an image with Pillow\n",
    "def read_image_file(file_name):\n",
    "    # Open the image form working directory\n",
    "    image = Image.open(os.path.join('.', 'datasets', file_name))\n",
    "    # summarize some details about the image\n",
    "    print(image.format)\n",
    "    print(image.size)\n",
    "    print(image.mode)\n",
    "    print(type(image))\n",
    "    new_size = (30, 40)\n",
    "    image = image.resize(new_size)\n",
    "    # convert image to numpy array\n",
    "    data = np.asarray(image)\n",
    "    print(type(data))\n",
    "    # summarize shape\n",
    "    print(data.shape)\n",
    "    #data = np.where(data==255, 1, data) \n",
    "    # print(find_unique_values(data))\n",
    "    # print(data[0:5])\n",
    "    #reduced_data = data[:, :, :1]\n",
    "    #reduced_data = reduced_data.reshape(30,40)\n",
    "    reduced_data = data\n",
    "    reduced_data = reduced_data.transpose(2,1,0)\n",
    "    print(find_unique_values(reduced_data))\n",
    "    return reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PNG\n",
      "(1200, 900)\n",
      "RGB\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 3)\n",
      "[  0   1   3   4   7  13  17  18  21  31  36  45  49  51  59  60  66  68\n",
      "  71  79  85  90  99 101 121 126 127 133 138 142 150 152 154 155 157 158\n",
      " 160 164 166 167 175 176 178 180 185 189 194 199 201 206 208 213 215 223\n",
      " 225 232 233 237 238 240 242 244 247 248 249 250 251 252 253 254 255]\n",
      "(3, 30, 40)\n",
      "[[[255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  ...\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]]\n",
      "\n",
      " [[255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  ...\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]]\n",
      "\n",
      " [[255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  ...\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]\n",
      "  [255 255 255 ... 255 255 255]]]\n"
     ]
    }
   ],
   "source": [
    "data = read_image_file(division[0]['train'][0])\n",
    "print(data.shape)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 10, 5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(9 * 14 * 10, 512)\n",
    "        self.fc2 = nn.Linear(512, 62)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define hyperparameters\n",
    "n_epochs = 1\n",
    "lr=0.01\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PNG\n",
      "(1200, 900)\n",
      "RGB\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 3)\n",
      "[  0   1   3   4   7  13  17  18  21  31  36  45  49  51  59  60  66  68\n",
      "  71  79  85  90  99 101 121 126 127 133 138 142 150 152 154 155 157 158\n",
      " 160 164 166 167 175 176 178 180 185 189 194 199 201 206 208 213 215 223\n",
      " 225 232 233 237 238 240 242 244 247 248 249 250 251 252 253 254 255]\n",
      "PNG\n",
      "(1200, 900)\n",
      "RGB\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 3)\n",
      "[  0   1   2   3   5   6   8   9  10  11  12  16  17  21  24  25  29  30\n",
      "  31  35  44  45  48  49  52  56  57  58  62  63  66  70  72  79  80  83\n",
      "  93  96 103 104 108 113 115 119 120 122 128 133 138 139 140 142 144 147\n",
      " 150 153 161 170 173 175 178 185 188 189 191 192 195 196 201 208 210 211\n",
      " 212 214 215 217 218 220 229 231 234 235 243 244 245 247 248 250 251 252\n",
      " 254 255]\n",
      "PNG\n",
      "(1200, 900)\n",
      "RGB\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 3)\n",
      "[  0  15  16  17  18  19  21  22  24  25  32  68  69  73  74  75  82  95\n",
      " 158 192 202 208 210 223 225 233 234 236 237 239 240 241 244 245 247 255]\n",
      "PNG\n",
      "(1200, 900)\n",
      "RGB\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 3)\n",
      "[  0   3   4   5  12  15  19  20  23  26  27  29  31  32  33  34  39  59\n",
      "  61  62  72  79  80  82  99 100 104 157 169 171 172 183 189 195 205 216\n",
      " 224 226 229 230 231 234 237 238 239 240 241 245 248 249 250 253 255]\n",
      "PNG\n",
      "(1200, 900)\n",
      "RGB\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 3)\n",
      "[  0   1   3   4   7  13  20  26  30  35  36  37  48  54  67  78  85  95\n",
      "  99 100 107 113 118 119 129 131 134 135 137 139 140 148 154 168 172 178\n",
      " 181 188 194 195 198 209 218 220 223 231 238 239 241 249 250 252 255]\n",
      "PNG\n",
      "(1200, 900)\n",
      "RGB\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "<class 'numpy.ndarray'>\n",
      "(40, 30, 3)\n",
      "[  0   2  13  14  15  16  23  26  27  35  37  43  44  48  67  69  70  72\n",
      "  74  88  91 110 118 120 121 122 131 136 147 148 151 152 156 160 167 172\n",
      " 177 193 201 208 211 218 224 225 234 239 241 242 243 244 245 249 250 251\n",
      " 252 253 254 255]\n",
      "6\n",
      "(3, 30, 40)\n"
     ]
    }
   ],
   "source": [
    "P_X = []\n",
    "P_Y = []\n",
    "t = [0, 1, 2]\n",
    "for i in range(0, len(t)):\n",
    "    key = t[i]\n",
    "    for j in range(0, 2):\n",
    "        f_n = division[key]['train'][j]\n",
    "        x = read_image_file(f_n)\n",
    "        P_X.append(x)\n",
    "        P_Y.append(key)\n",
    "print(len(P_X))\n",
    "print(P_X[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_X = torch.Tensor(P_X)\n",
    "P_Y = torch.Tensor(P_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 30, 40])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "print(P_X.shape)\n",
    "print(P_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chekcpoint(epoch, model_path, loss):\n",
    "    EPOCH = epoch\n",
    "    PATH = model_path\n",
    "    LOSS = loss\n",
    "\n",
    "    torch.save({\n",
    "                'epoch': EPOCH,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': LOSS,\n",
    "                }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(PATH):\n",
    "    if os.path.exists(PATH) is True:\n",
    "        model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=20, n_layers=1)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        checkpoint = torch.load(PATH)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        print(\"checkpoint loaded\")\n",
    "        print(checkpoint['model_state_dict'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        return epoch, loss, model, optimizer\n",
    "    else:\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_loss = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n"
     ]
    }
   ],
   "source": [
    "already_epoch, already_loss, model_, optimizer_ = load_checkpoint('model.pt')\n",
    "if model_ is not None:\n",
    "    model = model_\n",
    "    optimizer = optimizer_\n",
    "    model.train()\n",
    "print(already_epoch, already_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Run\n",
    "P_X = P_X.to(device)\n",
    "P_Y = P_Y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 11.4563,  17.2870,  -5.7760,  -5.0556,   7.0134,  -2.8845,   6.0410,\n",
      "         -12.2884, -10.0140,   7.3622, -28.0804,   2.8559, -14.3557,  19.4463,\n",
      "          -5.9991,  19.6588, -11.4869,  -6.8716,  -4.1353,   9.0117, -19.7673,\n",
      "          -7.0941,  -5.0945, -18.8516,   2.9312,   5.0161,  15.4688,  16.2699,\n",
      "           3.3200,  11.2652,  11.6246,   4.0709,   4.0014,  12.8889, -19.4137,\n",
      "          -6.2734,  -2.0828,  -1.6120, -10.6305,  -3.2199,   2.7645,   9.2637,\n",
      "          -4.7856,   8.2540,  -1.5966,  -6.7365, -12.3643,   8.0155, -10.6077,\n",
      "          17.8498,  -9.1107,  -9.1842,  19.6738,  24.7721,   1.3824,   2.6653,\n",
      "          20.0453,  13.7611,  -8.1565, -17.6495,   1.3668,  12.6914],\n",
      "        [ 10.6677,  20.2381,  -4.2943,  -7.7266,   3.6593,  -3.7612,   5.6832,\n",
      "          -6.5927,  -0.7473,   5.9200, -23.0827,   0.4124, -15.1251,  21.1060,\n",
      "          -1.7936,  12.9753, -11.2884,  -4.8272,  -5.6626,  11.5797, -13.7480,\n",
      "          -8.4664,  -6.8742, -17.7508,   3.8781,   4.1083,  10.7650,  10.6087,\n",
      "          -1.6963,  10.6918,  14.3422,   0.2334,  -0.9343,  11.5500, -16.1929,\n",
      "          -7.0604,   4.2712,  -1.9107, -16.1087,  -2.3748,  -0.8068,  10.8657,\n",
      "          -9.2359,  11.7102,   1.2561,  -2.3924,  -8.3284,   8.6077, -12.1409,\n",
      "           6.5912, -11.5968,  -5.4251,  21.2948,  33.1153,  -4.9696,   4.7641,\n",
      "          22.4779,  17.8493, -10.8672, -18.3743,   3.8717,  20.3763],\n",
      "        [  8.0040,  15.6210,  -5.1126,  -3.2513,   7.1147,   0.9188,   3.3906,\n",
      "          -8.4500,  -5.1567,   9.4940, -28.5932,   3.3079, -12.2902,  18.6638,\n",
      "          -3.1214,  16.6912,  -6.8885,  -2.7573,  -5.5057,  10.9468, -16.3208,\n",
      "          -7.0772,  -8.8135, -18.0556,   6.2018,   6.7674,  11.2053,  12.5335,\n",
      "          -3.6128,  12.7890,   9.8103,  -1.7498,   1.4903,  16.7735, -17.0265,\n",
      "          -4.1650,   0.2200,  -0.4076, -15.4897,   1.6879,   1.9926,   9.5657,\n",
      "          -9.3871,  10.5815,   2.0009,  -8.1171,  -8.9708,   9.1583,  -8.0796,\n",
      "          10.0350, -12.3207,  -7.6811,  19.5614,  32.2389,   0.5377,   0.2572,\n",
      "          22.9053,  16.4197,  -8.2324, -16.9538,   2.7483,  17.3701],\n",
      "        [ 12.7375,  16.1330,  -1.8659,  -2.3731,   2.9572,  -1.9673,   7.2060,\n",
      "         -10.6117,  -6.0185,   6.7362, -26.5809,   5.2747, -12.0062,  18.0563,\n",
      "          -3.6850,  19.3263, -14.8328,  -1.0899,  -7.4773,  12.3198, -23.4397,\n",
      "          -5.5609,  -5.7391, -16.5108,   5.5384,   4.8451,  12.5555,   8.4796,\n",
      "          -0.4318,  10.1807,   8.8950,   4.2945,   1.6133,  14.1227, -20.0053,\n",
      "          -2.4306,  -2.3982,  -4.2803, -12.2304,   0.8243,   4.6531,  11.7448,\n",
      "          -5.6787,   8.6931,  -1.2997,  -4.0801, -13.7239,   6.9026, -15.2430,\n",
      "          13.2158,  -7.5728,  -9.2361,  20.7726,  26.6122,  -1.9272,  -1.0704,\n",
      "          22.8508,  13.8472,  -9.7981, -19.0552,   4.5494,  15.4481],\n",
      "        [  8.8048,  19.4062,  -2.7907,  -5.9613,   2.6515,  -0.2733,   1.2647,\n",
      "          -7.5659,  -6.9362,   5.8461, -24.8797,   1.2740, -10.9093,  15.7876,\n",
      "          -3.7280,  16.8401, -13.8539,  -7.1787,  -6.0833,   7.6259, -17.2055,\n",
      "          -9.2135,  -6.7220, -16.8423,   7.7766,   3.3414,  11.3441,  10.4621,\n",
      "           0.9428,  15.1377,  12.1412,   1.3080,   0.3849,  12.8960, -15.3711,\n",
      "          -4.1944,   3.7919,  -2.2981, -12.3896,   0.2473,   2.0893,  11.7006,\n",
      "          -6.3669,  10.7618,  -3.7912,  -4.9409,  -8.0898,  11.3841,  -8.2438,\n",
      "           9.4216, -12.4806, -11.7232,  20.7537,  28.1690,  -1.7667,  -0.3888,\n",
      "          27.0617,  13.7893, -11.4614, -18.2870,   4.2459,  17.1643],\n",
      "        [  8.8457,  22.7564,  -3.5659,  -4.5284,   4.5276,  -1.5358,   4.2541,\n",
      "          -9.7806,  -5.1878,   6.7666, -25.5254,   4.3271,  -9.7638,  13.9345,\n",
      "          -5.4478,  20.6095, -10.4685,  -4.6153,  -8.7655,  11.7228, -18.6545,\n",
      "         -11.9014,  -8.8329, -16.2354,   5.9681,   4.5711,   7.2238,  11.8561,\n",
      "          -1.3410,  13.3416,  10.3186,  -0.8568,   2.1843,  13.1641, -14.0025,\n",
      "          -4.9034,   3.1518,  -0.7419, -12.2322,  -0.4349,   1.4907,  10.6579,\n",
      "          -7.9740,  11.3007,  -1.7297,  -6.7654,  -7.8156,   8.8722, -10.0427,\n",
      "           8.0412, -17.6421,  -6.2536,  20.7651,  32.0846,   0.6441,   1.3152,\n",
      "          26.2834,  16.0749,  -9.3655, -18.3018,   6.6031,  14.5508]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "Epoch: 1/1............. Loss: 21.6357\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    #input_seq = input_seq.to(device)\n",
    "    outputs = model(P_X)\n",
    "    outputs = outputs.to(device)\n",
    "    # print(outputs)\n",
    "    loss = criterion(outputs, P_Y.long())\n",
    "    loss.backward() # Does backpropagation and calculates gradients\n",
    "    optimizer.step() # Updates the weights accordingly\n",
    "    \n",
    "    if epoch%10 == 1:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1  2  3]\n",
      "  [ 4  5  6]]\n",
      "\n",
      " [[ 7  8  9]\n",
      "  [10 11 12]]\n",
      "\n",
      " [[13 14 15]\n",
      "  [16 17 18]]\n",
      "\n",
      " [[19 20 21]\n",
      "  [22 23 24]]]\n",
      "[[[ 1  7 13 19]\n",
      "  [ 4 10 16 22]]\n",
      "\n",
      " [[ 2  8 14 20]\n",
      "  [ 5 11 17 23]]\n",
      "\n",
      " [[ 3  9 15 21]\n",
      "  [ 6 12 18 24]]]\n"
     ]
    }
   ],
   "source": [
    "x = [[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]], [[13,14,15],[16,17,18]], [[19,20,21],[22,23,24]]]\n",
    "data = np.asarray(x)\n",
    "print(data)\n",
    "data2 = data.transpose(2,1,0)\n",
    "print(data2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
