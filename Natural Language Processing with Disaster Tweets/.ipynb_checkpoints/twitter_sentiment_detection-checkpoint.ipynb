{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tiger/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/tiger/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_name):\n",
    "    data = pd.read_csv(file_name)\n",
    "    print(data.head())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = os.path.join('datasets','train.csv')\n",
    "test_file = os.path.join('datasets','test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n",
      "   id keyword location                                               text\n",
      "0   0     NaN      NaN                 Just happened a terrible car crash\n",
      "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
      "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
      "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
      "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n"
     ]
    }
   ],
   "source": [
    "train_data = load_file(train_file)\n",
    "test_data = load_file(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonDataProcess:\n",
    "    def __init__(self):\n",
    "        self.stop_words = [w.lower() for w in stopwords.words('english')]\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def remove_digits(self, text):\n",
    "        text = re.sub('[0-9]',\"\",text)\n",
    "        return text\n",
    "    def lower_case(self, text):\n",
    "        return text.lower()\n",
    "    def word_tokenize_func(self, text):\n",
    "        text = word_tokenize(text)\n",
    "        return text\n",
    "    def lemmatize_func(self, words):\n",
    "        save = []\n",
    "        for w in words:\n",
    "            save.append(lemmatizer.lemmatize(w))\n",
    "        return save\n",
    "    def remove_stop_words(self, words):\n",
    "        save = []\n",
    "        for i in words:\n",
    "            if i not in self.stop_words:\n",
    "                save.append(i)\n",
    "        return save\n",
    "    def punctuation_remove(self, text):\n",
    "        text = re.sub(\"[!|\\\"|#|$|%|&|\\'|\\(|\\)|*|+|,|-|.|/|:|;|<|=|>|?|@|\\[|\\\\|\\]|^|_|\\`|\\{|\\||\\}|~]\", \" \", text)\n",
    "        return text\n",
    "    def process_text(self, text):\n",
    "        input_text = text\n",
    "        text = self.lower_case(text)\n",
    "        text = self.remove_digits(text)\n",
    "        text = self.punctuation_remove(text)\n",
    "        words = self.word_tokenize_func(text)\n",
    "        words = self.lemmatize_func(words)\n",
    "        words = self.remove_stop_words(words)\n",
    "        # print(words, input_text)\n",
    "        return words\n",
    "        \n",
    "comm = CommonDataProcess()\n",
    "# print(comm.process_text('i, study. in america? 911'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_corpus():\n",
    "    corpus = []\n",
    "    all_text = train_data['text'].tolist() + test_data['text'].tolist()\n",
    "    for i in range(len(all_text)):\n",
    "        clean = comm.process_text(all_text[i])\n",
    "        clean = \" \".join([w for w in clean])\n",
    "        clean = clean.strip()\n",
    "        corpus.append(clean)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tfidf(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    print(len(vectorizer.get_feature_names()))\n",
    "    print(X.shape)\n",
    "    return vectorizer, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10876\n",
      "7613 3263\n"
     ]
    }
   ],
   "source": [
    "corpus= make_corpus()\n",
    "print(len(corpus))\n",
    "print(len(train_data.index), len(test_data.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25393\n",
      "(10876, 25393)\n"
     ]
    }
   ],
   "source": [
    "vectorizer, X = apply_tfidf(corpus=corpus)\n",
    "X_arr = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7543   584 13313  5968 17813  4906]\n",
      "[0.47724964 0.42914156 0.30689576 0.3338231  0.35650945 0.50532014]\n",
      "[ 2966 18939 18527 12039 14546  7286  7534]\n",
      "[0.38575143 0.5002272  0.5002272  0.35261631 0.30611794 0.21457655\n",
      " 0.29436673]\n",
      "[ 6796 15667  6667 15337 14973 16508 19438  1140 18128]\n",
      "[0.2571909  0.22129414 0.21106286 0.2286107  0.33596837 0.45886817\n",
      " 0.56667504 0.26787685 0.26787685]\n",
      "[ 2914 23898 17839 16217 15667  6667]\n",
      "[0.34118418 0.3614614  0.58022411 0.30373414 0.41191709 0.39287259]\n",
      "15667 0.22129413712583343 0.4119170913801556\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-65cf74bfaf17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_idf_weight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_idf_weight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "idx_map = {}\n",
    "tf_idf_weight={}\n",
    "for w in vectorizer.vocabulary_:\n",
    "    idx_map[vectorizer.vocabulary_[w]] = w\n",
    "# print(idx_map)\n",
    "for row in X:\n",
    "    indices = row.indices\n",
    "    weight = row.data\n",
    "    print(indices)\n",
    "    print(weight)\n",
    "    for i in range(0, len(indices)):\n",
    "        if tf_idf_weight.get(indices[i]) is None:\n",
    "            tf_idf_weight[indices[i]] = weight[i]\n",
    "        else:\n",
    "            print(indices[i], tf_idf_weight[indices[i]], weight[i])\n",
    "            assert(tf_idf_weight[indices[i]] == weight[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "0       1\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "7608    1\n",
      "7609    1\n",
      "7610    1\n",
      "7611    1\n",
      "7612    1\n",
      "Name: target, Length: 7613, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(type(X_arr))\n",
    "X_train = X_arr[0:len(train_data.index)]\n",
    "X_test = X_arr[len(train_data.index):]\n",
    "Y_train = train_data['target']\n",
    "#text = X_arr[0:len(tes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "Y_test = clf.predict(X_test)\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2238, 1: 1025}\n"
     ]
    }
   ],
   "source": [
    "dic={0:0,1:0}\n",
    "for i in range(0, len(Y_test)):\n",
    "    dic[Y_test[i]] =  dic[Y_test[i]]+1\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_list = test_data['id']\n",
    "pd_data = {'id':res_list, 'target':Y_test}\n",
    "df = pd.DataFrame(pd_data)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
